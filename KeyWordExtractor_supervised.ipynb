{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from readability.readability import Document\n",
    "import operator\n",
    "from itertools import takewhile, tee, izip\n",
    "import networkx\n",
    "%matplotlib inline\n",
    "\n",
    "import mechanize\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from html2text import html2text \n",
    "import re\n",
    "import json\n",
    "import urllib2\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from os import walk\n",
    "from os import path\n",
    "import yaml\n",
    "from collections import OrderedDict, defaultdict\n",
    "import sys\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer #--for stemming\n",
    "from simhash import Simhash, SimhashIndex\n",
    "from hashlib import md5, sha1\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from nltk.stem.porter import *\n",
    "import itertools, nltk, string\n",
    "import random\n",
    "import collections, math, nltk, re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from os import walk\n",
    "from os import path\n",
    "from pprint import pprint\n",
    "from scipy.stats import sem # standard error of mean\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest, chi2, f_classif, f_regression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier, _predict_binary\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sys import maxint\n",
    "from time import time\n",
    "from scipy.cluster.vq import whiten\n",
    "import pdb\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_recall_curve, precision_score,recall_score,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from scipy import interp\n",
    "import urllib2\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_html(html):\n",
    "    \"\"\"\n",
    "    Copied from NLTK package.\n",
    "    Remove HTML markup from the given string.\n",
    "\n",
    "    :param html: the HTML string to be cleaned\n",
    "    :type html: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # First we remove inline JavaScript/CSS:\n",
    "    cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", html.strip())\n",
    "    # Then we remove html comments. This has to be done before removing regular\n",
    "    # tags since comments can contain '>' characters.\n",
    "    cleaned = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", cleaned)\n",
    "    # Next we can remove the remaining tags:\n",
    "    cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
    "    # Finally, we deal with whitespace\n",
    "    cleaned = re.sub(r\"&nbsp;\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def extract_text_html(url):\n",
    "    '''\n",
    "    Function to get the web page content and cleans it using readability package\n",
    "    '''\n",
    "    try:    \n",
    "        opener = urllib2.build_opener()\n",
    "        opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "        response = opener.open(url)\n",
    "        html_contents = response.read()\n",
    "        readable_article = Document(html_contents).summary()\n",
    "        readable_title = Document(html_contents).short_title()\n",
    "        cleanhtml = clean_html(readable_article)\n",
    "        text = html2text(cleanhtml)\n",
    "        return readable_title, text  \n",
    "    \n",
    "    except urllib2.HTTPError, error:\n",
    "        print error.read()\n",
    "        sys.exit(1)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # catastrophic error.\n",
    "        print e\n",
    "        sys.exit(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_to_pos(ch):\n",
    "    tokens = nltk.word_tokenize(ch)\n",
    "    return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "\n",
    "def is_good_POS(candidate):\n",
    "\n",
    "    tagged_sents = token_to_pos(candidate)\n",
    "    for pos in tagged_sents:\n",
    "        if pos == 'NN' or pos == 'NNS':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_number(candidate):\n",
    "    for s in nltk.word_tokenize(candidate):\n",
    "        try:\n",
    "            float(s) if '.' in s else int(s)\n",
    "        except ValueError:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def load_stop_words(stop_word_file):\n",
    "    \"\"\"\n",
    "    Utility function to load stop words from a file and return as a list of words\n",
    "    @param stop_word_file Path and file name of a file containing stop words.\n",
    "    @return list A list of stop words.\n",
    "    \"\"\"\n",
    "    stop_words = []\n",
    "    for line in open(stop_word_file):\n",
    "        if line.strip()[0:1] != \"#\":\n",
    "            for word in line.split():  # in case more than one per line\n",
    "                stop_words.append(word)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\"\n",
    "    returns a list of sentences.\n",
    "    @param text The text that must be split in to sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[.!?,;:\\t\\\\\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013\\n]|\\\\s\\\\-\\\\s')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def build_stop_word_regex(stop_word_file_path):\n",
    "    stop_word_list = load_stop_words(stop_word_file_path)\n",
    "    stop_word_regex_list = []\n",
    "    for word in stop_word_list:\n",
    "        word_regex = r'\\b' + word + r'(?![\\w-])'  # added look ahead for hyphen\n",
    "        stop_word_regex_list.append(word_regex)\n",
    "    stop_word_pattern = re.compile('|'.join(stop_word_regex_list), re.IGNORECASE)\n",
    "    return stop_word_pattern\n",
    "\n",
    "\n",
    "def generate_candidate_keywords(sentence_list, stopword_pattern):\n",
    "    phrase_list = []\n",
    "    phrase_list2 = []\n",
    "    punct = set(string.punctuation)\n",
    "    for s in sentence_list:\n",
    "        tmp = re.sub(stopword_pattern, '|', s.strip())\n",
    "        phrases = tmp.split(\"|\")\n",
    "        for phrase in phrases:\n",
    "            phrase = phrase.strip().lower()\n",
    "            if phrase != \"\":\n",
    "                noPunct = removePunctuations(phrase)\n",
    "                phrase_list.append(noPunct)\n",
    "    \n",
    "    for phrase in phrase_list:\n",
    "        splitted = phrase.split()\n",
    "        if is_number(phrase) is False:\n",
    "            if (len(splitted) > 3):  #if the length of the phrase is > 3, split into 2-grams\n",
    "                for i in range (0,len(splitted)-2):\n",
    "                    cand = phrase.split()[i]+\" \" +phrase.split()[i+1]+\" \"+phrase.split()[i+2]\n",
    "                    if is_good_POS(cand):\n",
    "                        phrase_list2.append(cand)\n",
    "            else:\n",
    "                if is_good_POS(phrase):\n",
    "                    phrase_list2.append(phrase)\n",
    "    \n",
    "    return phrase_list2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removePunctuations (phrase):\n",
    "    # define punctuation\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    no_punct = \"\"\n",
    "    for char in phrase:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "\n",
    "    return no_punct\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_words(text, min_word_return_size):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of all words that are have a length greater than a specified number of characters.\n",
    "    @param text The text that must be split in to words.\n",
    "    @param min_word_return_size The minimum no of characters a word must have to be included.\n",
    "    \"\"\"\n",
    "    splitter = re.compile('[^a-zA-Z0-9_\\\\+\\\\-/]')\n",
    "    words = []\n",
    "    for single_word in splitter.split(text):\n",
    "        current_word = single_word.strip().lower()\n",
    "        #leave numbers in phrase, but don't count as words, since they tend to invalidate scores of their phrases\n",
    "        if len(current_word) > min_word_return_size and current_word != '' and not is_number(current_word):\n",
    "            words.append(current_word)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_word_scores(phraseList):\n",
    "    word_frequency = {}\n",
    "    word_degree = {}\n",
    "    for phrase in phraseList:\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        word_list_length = len(word_list)\n",
    "        word_list_degree = word_list_length - 1\n",
    "        #if word_list_degree > 3: word_list_degree = 3 #exp.\n",
    "        for word in word_list:\n",
    "            word_frequency.setdefault(word, 0)\n",
    "            word_frequency[word] += 1\n",
    "            word_degree.setdefault(word, 0)\n",
    "            word_degree[word] += word_list_degree  #orig.\n",
    "            #word_degree[word] += 1/(word_list_length*1.0) #exp.\n",
    "    for item in word_frequency:\n",
    "        word_degree[item] = word_degree[item] + word_frequency[item]\n",
    "\n",
    "    # Calculate Word scores = deg(w)/frew(w)\n",
    "    word_score = {}\n",
    "    for item in word_frequency:\n",
    "        word_score.setdefault(item, 0)\n",
    "        word_score[item] = word_degree[item] / (word_frequency[item] * 1.0)  #orig.\n",
    "    #word_score[item] = word_frequency[item]/(word_degree[item] * 1.0) #exp.\n",
    "    return word_score\n",
    "\n",
    "\n",
    "def generate_candidate_keyword_scores(phrase_list, word_score):\n",
    "    keyword_candidates = {}\n",
    "    for phrase in phrase_list:\n",
    "        keyword_candidates.setdefault(phrase, 0)\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        candidate_score = 0\n",
    "        for word in word_list:\n",
    "            candidate_score += word_score[word]\n",
    "        keyword_candidates[phrase] = candidate_score\n",
    "    return keyword_candidates\n",
    "\n",
    "\n",
    "class Rake(object):\n",
    "       def run(self, phrase_list):\n",
    "\n",
    "        word_scores = calculate_word_scores(phrase_list)\n",
    "\n",
    "        keyword_candidates = generate_candidate_keyword_scores(phrase_list, word_scores)\n",
    "\n",
    "        sorted_keywords = sorted(keyword_candidates.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "        return sorted_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank Algorithm-- builds graph of connected words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_candidate_words(text, good_tags=set(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])):\n",
    "    import itertools, nltk, string\n",
    "\n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize and POS-tag words\n",
    "    tagged_words = itertools.chain.from_iterable(nltk.pos_tag_sents(nltk.word_tokenize(sent)\n",
    "                                                                    for sent in nltk.sent_tokenize(text)))\n",
    "    # filter on certain POS tags and lowercase all words\n",
    "    candidates = [word.lower() for word, tag in tagged_words\n",
    "                  if tag in good_tags and word.lower() not in stop_words\n",
    "                  and not all(char in punct for char in word)]\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def score_keyphrases_by_textrank(text, n_keywords=0.05):\n",
    "       \n",
    "    # tokenize for all words\n",
    "    words = [word.lower()\n",
    "             for sent in nltk.sent_tokenize(text)\n",
    "             for word in nltk.word_tokenize(sent)]\n",
    "    candidates = extract_candidate_words(text)\n",
    "    # build graph, each node is a unique candidate\n",
    "    graph = networkx.Graph()\n",
    "    graph.add_nodes_from(set(candidates))\n",
    "    # iterate over word-pairs, add unweighted edges into graph\n",
    "    def pairwise(iterable):\n",
    "        \"\"\"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\"\"\n",
    "        a, b = tee(iterable)\n",
    "        next(b, None)\n",
    "        return izip(a, b)\n",
    "    for w1, w2 in pairwise(candidates):\n",
    "        if w2:\n",
    "            graph.add_edge(*sorted([w1, w2]))\n",
    "    # score nodes using default pagerank algorithm, sort by score, keep top n_keywords\n",
    "    ranks = networkx.pagerank(graph)\n",
    "    if 0 < n_keywords < 1:\n",
    "        n_keywords = int(round(len(candidates) * n_keywords))\n",
    "    word_ranks = {word_rank[0]: word_rank[1]\n",
    "                  for word_rank in sorted(ranks.iteritems(), key=lambda x: x[1], reverse=True)[:n_keywords]}\n",
    "    keywords = set(word_ranks.keys())\n",
    "    # merge keywords into keyphrases\n",
    "    keyphrases = {}\n",
    "    j = 0\n",
    "    for i, word in enumerate(words):\n",
    "        if i < j:\n",
    "            continue\n",
    "        if word in keywords:\n",
    "            kp_words = list(takewhile(lambda x: x in keywords, words[i:i+10]))\n",
    "            avg_pagerank = sum(word_ranks[w] for w in kp_words) / float(len(kp_words))\n",
    "            keyphrases[' '.join(kp_words)] = avg_pagerank\n",
    "            # counter as hackish way to ensure merged keyphrases are non-overlapping\n",
    "            j = i + len(kp_words)\n",
    "    \n",
    "    return sorted(keyphrases.iteritems(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "class TextRank(object):\n",
    "    def run(self,text):\n",
    "        keywords = score_keyphrases_by_textrank(text, 0.4 )\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generating training and test feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_dir_list(dir):\n",
    "    '''\n",
    "    Get a list of directories and files. Used to get the corpora.\n",
    "    Returns\n",
    "    -------\n",
    "    dir_list: list of directory names to serve as class labels.\n",
    "    file_list: list of files in corpus.\n",
    "    '''\n",
    "\n",
    "    file_list = []\n",
    "    dir_list = []\n",
    "    for (dirpath, dirname, files) in walk(dir):\n",
    "        if files:\n",
    "            dir_list.append(path.split(dirpath)[1])\n",
    "            file_list.append(map(lambda x: path.join(dirpath, x), files))\n",
    "    return dir_list, file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_stop_words(stop_word_file):\n",
    "    \"\"\"\n",
    "    function to load stop words from a file and return as a list of words\n",
    "    @param stop_word_file Path and file name of a file containing stop words.\n",
    "    @return list A list of stop words.\n",
    "    \"\"\"\n",
    "    stop_words = []\n",
    "    for line in open(stop_word_file):\n",
    "        if line.strip()[0:1] != \"#\":\n",
    "            for word in line.split():  # in case more than one per line\n",
    "                stop_words.append(word)\n",
    "    return stop_words\n",
    "\n",
    "def tokenize(str):\n",
    "    tokens = nltk.word_tokenize(str)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def generate_negative_keywords(word_tokens,  candidates, title):\n",
    "    phrase_list = []\n",
    "    #load Stop words, concatenate with candidates and filter against\n",
    "    stop_words_path = \"SmartStoplist.txt\"\n",
    "    stopWords = load_stop_words(stop_words_path)\n",
    "    notNegList = stopWords + candidates + tokenize(title)\n",
    "    notNegSet = set(notNegList)\n",
    "        \n",
    "    # define punctuation\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "    negatives = []\n",
    "    # remove punctuation from the string\n",
    "    for word in word_tokens:\n",
    "        if word not in notNegSet: \n",
    "            no_punct = \"\"\n",
    "            for char in word:\n",
    "                if char not in punctuations:\n",
    "                    no_punct = no_punct + char\n",
    "        \n",
    "            if no_punct != \"\" and len(no_punct) >= 3:\n",
    "                negatives.append(no_punct)\n",
    "    return negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_candidate_features(candidates, doc_text,  doc_title): #doc_excerpt in abstract or introductory paragraph\n",
    "    \n",
    "    \n",
    "    candidate_scores = collections.OrderedDict()\n",
    "    \n",
    "    # get word counts for document\n",
    "    doc_word_counts = collections.Counter(word.lower()\n",
    "                                          for sent in nltk.sent_tokenize(doc_text)\n",
    "                                          for word in nltk.word_tokenize(sent))\n",
    "    \n",
    "    features = collections.OrderedDict()\n",
    "    \n",
    "    \n",
    "    for candidate in candidates:\n",
    "        if candidate != \"\" and len(candidate) >2:\n",
    "            pattern = re.compile(re.escape(candidate), re.IGNORECASE)\n",
    "            # frequency-based\n",
    "            # number of times candidate appears in document\n",
    "            cand_doc_count = len(pattern.findall(doc_text))\n",
    "            doc_text_length = float(len(doc_text))\n",
    "            cand_doc_count_normalized =cand_doc_count /  doc_text_length\n",
    "            if not cand_doc_count:\n",
    "                #print '**WARNING:', candidate, 'not found!'\n",
    "                #print doc_text\n",
    "                continue\n",
    "\n",
    "            # statistical\n",
    "            candidate_words = candidate.split()\n",
    "            max_word_length = max(len(w) for w in candidate_words)\n",
    "            term_length = len(candidate_words)\n",
    "            # get frequencies for term and constituent words\n",
    "            sum_doc_word_counts = float(sum(doc_word_counts[w] for w in candidate_words))\n",
    "            try:\n",
    "                # lexical cohesion doesn't make sense for 1-word terms\n",
    "                if term_length == 1:\n",
    "                    lexical_cohesion = 0.0\n",
    "                else:\n",
    "                    lexical_cohesion = term_length * (1 + math.log(cand_doc_count, 10)) * cand_doc_count / sum_doc_word_counts\n",
    "            except (ValueError, ZeroDivisionError) as e:\n",
    "                lexical_cohesion = 0.0\n",
    "\n",
    "            # positional\n",
    "            # found in title\n",
    "            in_title_sum=0\n",
    "            for w in candidate_words:\n",
    "                in_title_sum = in_title_sum + (1 if re.search(w, doc_title) else 0)\n",
    "\n",
    "            # first/last position, difference between them (spread)\n",
    "            first_match = pattern.search(doc_text)\n",
    "            abs_first_occurrence = first_match.start() / doc_text_length\n",
    "            if cand_doc_count == 1:\n",
    "                spread = 0.0\n",
    "                abs_last_occurrence = abs_first_occurrence\n",
    "            else:\n",
    "                for last_match in pattern.finditer(doc_text):\n",
    "                    pass\n",
    "                abs_last_occurrence = last_match.start() / doc_text_length\n",
    "                spread = abs_last_occurrence - abs_first_occurrence\n",
    "\n",
    "            candidate_scores[candidate] = [ cand_doc_count_normalized, term_length,  max_word_length, spread,  \n",
    "                                           lexical_cohesion, in_title_sum, abs_first_occurrence, abs_last_occurrence]\n",
    "\n",
    "    return candidate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_features_from_corpus( file_list):\n",
    "    '''\n",
    "    Parse each text file and generate its features.\n",
    "    '''\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    count = 0\n",
    "    totalwords = 0\n",
    "    for index, files in enumerate(file_list):\n",
    "       \n",
    "        for f in files:\n",
    "            if f.endswith('-justTitle.txt'):\n",
    "                #count = count +1\n",
    "                #if count == 100:\n",
    "                #    break\n",
    "                titleFile = f\n",
    "                textFile = f.split('-justTitle.txt')[0]+\".txt\"\n",
    "                candidateFile = f.split('-justTitle.txt')[0]+\".key\"\n",
    "            \n",
    "                text = open(textFile).read().decode('utf-8')\n",
    "                title = open(titleFile).read().decode('utf-8')\n",
    "                candidates= open(candidateFile).read().decode('utf-8').split('\\n')\n",
    "                if(len(candidates)>=1):\n",
    "                    #generate features for candidates\n",
    "                    \n",
    "                    candidate_score = extract_candidate_features(candidates, text, title)\n",
    "\n",
    "                    #get some negative words, and make sure they are not in the candidates\n",
    "                    nonCandidates = generate_negative_keywords (tokenize(text),candidates, title) \n",
    "                    negative=[]\n",
    "                    for i in range(0,len(candidates)):\n",
    "                        negative.append( random.choice(nonCandidates))\n",
    "\n",
    "                    #generate features for the negative examples\n",
    "                    negative_score = extract_candidate_features(negative, text, title)\n",
    "\n",
    "                    #generate matrix\n",
    "                    instance_X = np.concatenate((candidate_score.values(), negative_score.values()), axis=0)\n",
    "                    instance_y_pos = np.ones(len(candidate_score))\n",
    "                    instance_y_n = np.ones(len(negative_score))\n",
    "                    instance_y_neg = np.negative(instance_y_n)\n",
    "                    instance_Y = np.concatenate((instance_y_pos, instance_y_neg), axis=0)\n",
    "\n",
    "                    X.extend(instance_X)\n",
    "                    Y.extend(instance_Y)\n",
    "    return np.array(X), np.array(Y)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateFeatures(FEATURESFILE):\n",
    "    '''This function generates feature file'''\n",
    "    CorpusPath = \"/Users/Dina/Desktop/Insight/Data_Challenges/BrightEdge/500N-KPCrowd-v1.1/CorpusAndCrowdsourcingAnnotations/train/\"\n",
    "    dir_list, file_list = get_file_dir_list(CorpusPath)\n",
    "    x, y = load_features_from_corpus(file_list)\n",
    "    save_book_features_to_file(FEATURESFILE,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_book_features_to_file(FEATURESFILE, x, y):\n",
    "    '''\n",
    "    Save book features to a features file.\n",
    "    '''\n",
    "\n",
    "    f = open(FEATURESFILE, 'wb')\n",
    "    for index, item in enumerate(x):\n",
    "        f.write(\"%d\\t%s\\n\" % ( y[index], ', '.join(map(str, item))))\n",
    "    f.close()\n",
    "\n",
    "    print 'Features saved to file %s' % FEATURESFILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_features_from_file(FEATURESFILE):\n",
    "    '''\n",
    "    Parse a previously created features file and load features for all the corpus.\n",
    "    '''\n",
    "\n",
    "    contents = open(FEATURESFILE, 'rb').read().strip().split('\\n')\n",
    "    x = []\n",
    "    y = []\n",
    "    authors = []\n",
    "    for line in contents:\n",
    "        l = line.split('\\t')\n",
    "        y.append(int(l[0]))\n",
    "        x.append(map(float, l[1].split(',')))\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Supervised ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crossValidation(x,y):\n",
    "    clf = RandomForestClassifier(max_depth=10, n_estimators=50, max_features='auto', class_weight='auto') \n",
    "    cv = StratifiedShuffleSplit(y, n_iter=4, test_size=0.25, random_state=0)\n",
    "    scores = cross_validation.cross_val_score(clf, x, y, cv=cv,  scoring='f1')\n",
    "    print \"Random Forest mean f1 is %.3f \" % (scores.mean())\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateClassificationReport(x,y):\n",
    "    clf = RandomForestClassifier(max_depth=10, n_estimators=50, max_features='auto', class_weight='auto') \n",
    "    cv = StratifiedShuffleSplit(y, n_iter=4, test_size=0.25, random_state=0)\n",
    "\n",
    "    for train_index, test_index in cv:\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_predict_test = clf.predict(X_test)\n",
    "\n",
    "        print \"\\nClassification Report:\"\n",
    "        print metrics.classification_report(y_test, y_predict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TrainClassifier(x,y):    \n",
    "    clf = RandomForestClassifier(max_depth=10, n_estimators=50, max_features='auto', class_weight='auto') \n",
    "    cv = StratifiedShuffleSplit(y, n_iter=1, test_size=0.25, random_state=0)\n",
    "\n",
    "    for train_index, test_index in cv:\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SupervisedModel(object):\n",
    "    def __init__(self, FEATURESFILE):\n",
    "        self.FEATURESFILE = FEATURESFILE\n",
    "       \n",
    "    def run(self,phrase_list, text):\n",
    "        if not path.exists(self.FEATURESFILE):\n",
    "            print 'Feature file not found. Will need to create it'\n",
    "            print 'this will take some time'\n",
    "            generateFeatures(FEATURESFILE)\n",
    "        else:\n",
    "            print 'Feature file found. Reading...'\n",
    "            x, y = load_features_from_file(self.FEATURESFILE)\n",
    "            #clf = crossValidation(x,y)\n",
    "            #train a classifier on the trian corpus\n",
    "            classifier = TrainClassifier(x,y)\n",
    "            #generate feature matrix for the candidates\n",
    "            candidateMatrix = extract_candidate_features(phrase_list, text,  title)\n",
    "            #classify\n",
    "            candidate_prediction = classifier.predict_proba(candidateMatrix.values())\n",
    "            #select from the candidate list those with positive label\n",
    "            y_prob = classifier.predict_proba(candidateMatrix.values())[:,1]\n",
    "            ind = np.where( y_prob> 0.5)[0]\n",
    "            prob = list(np.array(y_prob[ind]))\n",
    "            keywords = list(np.array(candidateMatrix.keys())[ind])\n",
    "            rankedKeywords = [x for (y,x) in sorted(zip(prob,keywords),reverse=True)]\n",
    "            output = np.concatenate((rankedKeywords, np.sort(prob)[::-1]), axis=0)\n",
    "            rankScore = np.sort(prob)[::-1]\n",
    "            #for i in range(0, len(rankedKeywords) ):\n",
    "            #    print rankedKeywords[i], rankScore[i]\n",
    "\n",
    "        return rankedKeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.60      0.75      0.67      3888\n",
      "        1.0       0.76      0.62      0.68      5071\n",
      "\n",
      "avg / total       0.69      0.67      0.67      8959\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.61      0.73      0.66      3888\n",
      "        1.0       0.76      0.64      0.69      5071\n",
      "\n",
      "avg / total       0.69      0.68      0.68      8959\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.59      0.79      0.67      3888\n",
      "        1.0       0.78      0.58      0.66      5071\n",
      "\n",
      "avg / total       0.70      0.67      0.67      8959\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.60      0.77      0.68      3888\n",
      "        1.0       0.78      0.61      0.68      5071\n",
      "\n",
      "avg / total       0.70      0.68      0.68      8959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generateClassificationReport(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'matplotlib.pyplot' from '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/pyplot.pyc'>"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEZCAYAAACAZ8KHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGQ7DMAwjK/4/pqAjO7yyHiYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c0abb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "title = \"Learning Curves (Random forest)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = cross_validation.ShuffleSplit(x.shape[0], n_iter=10,\n",
    "                                   test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = RandomForestClassifier(max_depth=5, n_estimators=50, max_features='auto', class_weight='auto') \n",
    "#SVC(kernel='linear') \n",
    "plot_learning_curve(estimator, title, x, y, ylim=(0.5, 1.01), cv=cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and traning learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : integer, cross-validation generator, optional\n",
    "        If an integer is passed, it is the number of folds (defaults to 3).\n",
    "        Specific cross-validation objects can be passed, see\n",
    "        sklearn.cross_validation module for the list of possible objects\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    AlgorithmID = 2;\n",
    "    noOfTopics = 10;\n",
    "    url = \"\";\n",
    "    if len(sys.argv) < 2:\n",
    "        print \"Input:<program name> url (optional:AlgorithmID noOfTopics)\"\n",
    "        print \"AlgorithmID: 1 = RAkE\"\n",
    "        print \"             2 = Supervised Learning Model\"\n",
    "        print \"             3 = Text Rank\"\n",
    "    else:\n",
    "        url = sys.argv[0]\n",
    "        if len(sys.argv) == 2 :   \n",
    "            AlgorithmID = sys.argv[1]\n",
    "        if len(sys.argv) == 3 :   \n",
    "            AlgorithmID = sys.argv[1]\n",
    "            noOfTopics = sys.argv[2]\n",
    "            \n",
    "    #extract meaningful content from html file\n",
    "    title, text = extract_text_html(url)\n",
    "    allText = title + \"\\n\" + text\n",
    "    #get candidate words\n",
    "    stop_words_path = \"SmartStoplist.txt\"\n",
    "    stop_words_pattern = build_stop_word_regex(stop_words_path)\n",
    "    sentence_list = split_sentences(text)\n",
    "    phrase_list = generate_candidate_keywords(sentence_list, stop_words_pattern)\n",
    "    #get candidate words from title\n",
    "    sentence_list = split_sentences(title)\n",
    "    phrase_list_title = generate_candidate_keywords(sentence_list, stop_words_pattern)\n",
    "    phrase_list = phrase_list + phrase_list_title\n",
    "\n",
    "    if AlgorithmID == 1:\n",
    "        #1. run RAKE algorithm\n",
    "        print \"Running algorithm RAKE\"\n",
    "        rake = Rake()\n",
    "        keywords = rake.run(phrase_list)\n",
    "        for (elem,prob) in keywords[:noOfTopics]:\n",
    "            print elem \n",
    "            \n",
    "    elif AlgorithmID == 3:\n",
    "        #2. run TextRank algorithm\n",
    "        print \"Running algorithm TextRank\"\n",
    "        textRank = TextRank()\n",
    "        keywords = textRank.run(allText)\n",
    "\n",
    "        for (elem,prob) in keywords[:noOfTopics]:\n",
    "            print elem\n",
    "    \n",
    "    elif AlgorithmID == 2:\n",
    "        print \"Running algorithm Supervised Model\"\n",
    "        FEATURESFILE = \"features.csv\"\n",
    "        sm = SupervisedModel(FEATURESFILE)\n",
    "        keywords = sm.run(phrase_list, allText)\n",
    "        for elem in keywords[:noOfTopics]:\n",
    "            print elem\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running algorithm RAKE\n",
      "president barack obama\n",
      "secrets wide open\n",
      "overactive mother teresa\n",
      "cell phone calls\n",
      "track cell phone\n",
      "global speculation sunday\n",
      "mother teresa gene\n",
      "high school diploma\n",
      "ultimate destination unknown\n",
      "barack obama insists\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        sys.argv[0] = \"http://www.cnn.com/2013/06/10/politics/edward-snowden-profile\"\n",
    "\n",
    "        sys.argv[1] = 1\n",
    "        sys.argv[2] = 10\n",
    "        main(sys.argv)\n",
    "    except:\n",
    "        print \"Unexpected error:\", sys.exc_info()[0]\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
